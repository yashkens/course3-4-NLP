{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa8d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import RAKE\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from summa import keywords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808169c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af0257",
   "metadata": {},
   "source": [
    "### Подготовка корпуса\n",
    "\n",
    "Все тексты взяты с habr.com, в качестве ключевых слов использованы теги, проставленные авторами текстов. Всего в корпусе 6 текстов.\n",
    "\n",
    "Получилось так, что эталонных ключевых слов как правило больше, чем слов из источника. Зачастую эталонные ключевые слова и слова из источника пересекаются, но я реже включала имена собственные (например, названия фирм или университетов) в свой список. Я не включала в свой список и те слова, которые не встречаются в тексте, но которые автор выделил как тег."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab80b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    texts = text.split('\\n\\n')\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2261c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split('\\n')\n",
    "    keywords = []\n",
    "    my_keywords = []\n",
    "    for line in lines:\n",
    "        keywords.append(line.split(';')[0])\n",
    "        my_keywords.append(line.split(';')[1])\n",
    "    return keywords, my_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5f6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_keywords(keyword_line):\n",
    "    lemmatized = []\n",
    "    keywords = keyword_line.split(',')\n",
    "    for word in keywords:\n",
    "        parts = word.split(' ')\n",
    "        norm_parts = [m.parse(p)[0].normal_form for p in parts]\n",
    "        lemmatized.append(' '.join(norm_parts))\n",
    "    return ','.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f967b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>my_keywords</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>По сведениям, которые интернет-издательство Bl...</td>\n",
       "      <td>распознавание голоса,голосовые ассистенты,расп...</td>\n",
       "      <td>amazon,патент,эмоция,голос</td>\n",
       "      <td>habr.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Недавно на шоссе меня подрезал таксист. Я без ...</td>\n",
       "      <td>языки,родной язык,иностранный язык,эмоции,логи...</td>\n",
       "      <td>язык,эмоция,ругательство,билингвизм,родный язы...</td>\n",
       "      <td>habr.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Создатели робота-музыканта Shimon анонсировали...</td>\n",
       "      <td>роботы,робототехника,университет джорджии,geor...</td>\n",
       "      <td>робот,музыка,песня,shimon,альбом</td>\n",
       "      <td>habr.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouTube удалил из открытого доступа ряд видеоз...</td>\n",
       "      <td>роботы,битвы роботов,цензура,эмпатия,сочувствие</td>\n",
       "      <td>робот,битва,цензура,эмпатия,сочувствие,youtube</td>\n",
       "      <td>habr.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Идея сделать робота максимально похожим на чел...</td>\n",
       "      <td>Toshiba,робот,андроид,искусственный интеллект</td>\n",
       "      <td>робот,человекоподобный робот,антропоморфный ро...</td>\n",
       "      <td>habr.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  По сведениям, которые интернет-издательство Bl...   \n",
       "1  Недавно на шоссе меня подрезал таксист. Я без ...   \n",
       "2  Создатели робота-музыканта Shimon анонсировали...   \n",
       "3  YouTube удалил из открытого доступа ряд видеоз...   \n",
       "4  Идея сделать робота максимально похожим на чел...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  распознавание голоса,голосовые ассистенты,расп...   \n",
       "1  языки,родной язык,иностранный язык,эмоции,логи...   \n",
       "2  роботы,робототехника,университет джорджии,geor...   \n",
       "3    роботы,битвы роботов,цензура,эмпатия,сочувствие   \n",
       "4      Toshiba,робот,андроид,искусственный интеллект   \n",
       "\n",
       "                                         my_keywords    source  \n",
       "0                         amazon,патент,эмоция,голос  habr.com  \n",
       "1  язык,эмоция,ругательство,билингвизм,родный язы...  habr.com  \n",
       "2                   робот,музыка,песня,shimon,альбом  habr.com  \n",
       "3     робот,битва,цензура,эмпатия,сочувствие,youtube  habr.com  \n",
       "4  робот,человекоподобный робот,антропоморфный ро...  habr.com  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_text('habr_texts.txt')\n",
    "source_keywords, my_keywords = get_keywords('habr_tags.txt')\n",
    "my_keywords = [lemmatize_keywords(words) for words in my_keywords]\n",
    "df = pd.DataFrame(texts, columns = ['text'])\n",
    "df['keywords'] = source_keywords\n",
    "df['my_keywords'] = my_keywords\n",
    "df['source'] = ['habr.com']*len(texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07395aad",
   "metadata": {},
   "source": [
    "Тексты и эталонные ключевые слова лемматизированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f396c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(m.parse(t)[0].normal_form)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf41cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_texts = [lemmatize(text) for text in df['text'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae3add",
   "metadata": {},
   "source": [
    "### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c5f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d25f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_words = [rake.run(lemma_text, maxWords=2, minFrequency=2) for lemma_text in lemma_texts]\n",
    "\n",
    "for i, words in enumerate(rake_words):\n",
    "    rake_words[i] = [word[0] for word in rake_words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3df7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake: человекоподобный робот,уметь ходить,робот,уметь,айко,похожий,человек,u,r,рассказывать,работать,телевокс,представить,глаз,—\n",
      "Эталон: робот,человекоподобный робот,антропоморфный робот,история,технология,внешность,функция\n"
     ]
    }
   ],
   "source": [
    "print('Rake:', ','.join(rake_words[4]))\n",
    "print('Эталон:', df['my_keywords'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcab22",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cf752b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_words = [keywords.keywords(lemma_text, language='russian', \n",
    "                  additional_stopwords=stop_words, scores=True, ratio=0.05) for lemma_text in lemma_texts]\n",
    "\n",
    "for i, words in enumerate(textrank_words):\n",
    "    textrank_words[i] = [word[0] for word in textrank_words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bec483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank: робот,человек,годиться,год,это,очень,уровень,телевокс мочь,способный,atlas,телевокса,движение,стационарный,статья,стать,айко,любой,honda,голос,человеческий,самый,современный,современность,мимика,антропоморфность,антропоморфный,универсальный\n",
      "Эталон: робот,человекоподобный робот,антропоморфный робот,история,технология,внешность,функция\n"
     ]
    }
   ],
   "source": [
    "print('TextRank:', ','.join(textrank_words[4]))\n",
    "print('Эталон:', df['my_keywords'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c045533",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Включаю биграммы, беру только top-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f5bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))\n",
    "tfidf = vectorizer.fit_transform(lemma_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8281da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_keyword(vectors):\n",
    "    features = np.array(vectorizer.get_feature_names())\n",
    "    sorted_indices = np.argsort(vectors.toarray()).ravel()[::-1]\n",
    "    return features[sorted_indices][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "795ca04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_words = []\n",
    "for i in range(len(lemma_texts)):\n",
    "    keywords = get_tfidf_keyword(tfidf[i])\n",
    "    tfidf_words.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4019a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-Idf: робот,человек,человекоподобный робот,человекоподобный,год,очень,уметь,похожий человек,айко,универсальный\n",
      "Эталон: робот,человекоподобный робот,антропоморфный робот,история,технология,внешность,функция\n"
     ]
    }
   ],
   "source": [
    "print('Tf-Idf:', ','.join(tfidf_words[4]))\n",
    "print('Эталон:', df['my_keywords'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20dc44",
   "metadata": {},
   "source": [
    "### Извлечение частеречных шаблонов\n",
    "\n",
    "Среди эталонных слов не такое большое разнообразие сочетаний. Встречаются существительные, слова, записанные латиницей и сочетания прилагательных с существительными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58557449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_pattern(word_line):\n",
    "    tokens = word_line.split(' ')\n",
    "    pos = [m.parse(t)[0].tag.POS if 'LATN' not in m.parse(t)[0].tag else 'LATN' for t in tokens]\n",
    "    if None in pos:\n",
    "        return ''\n",
    "    return '+'.join(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7abb67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_keyword_patterns(df):\n",
    "    pos_combos = []\n",
    "    for keywords in df['my_keywords'].values:\n",
    "        lines = keywords.split(',')\n",
    "        for line in lines:\n",
    "            pos_combos.append(get_pos_pattern(line))\n",
    "    return set(pos_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9395a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJF+NOUN', 'LATN', 'NOUN'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_patterns = get_gold_keyword_patterns(df)\n",
    "pos_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0e87e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_with_pattern(keywords, pos_patterns):\n",
    "    filtered = []\n",
    "    for line in keywords:\n",
    "        pos = get_pos_pattern(line)\n",
    "        if pos in pos_patterns:\n",
    "            filtered.append(line)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27c3f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_filtered = [filter_with_pattern(words, pos_patterns) for words in rake_words]\n",
    "textrank_filtered = [filter_with_pattern(words, pos_patterns) for words in textrank_words]\n",
    "tfidf_filtered = [filter_with_pattern(words, pos_patterns) for words in tfidf_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef6f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake filtered: человекоподобный робот,робот,айко,человек,u,r,телевокс,глаз \n",
      "\n",
      "TextRank filtered: робот,человек,год,уровень,atlas,телевокса,движение,статья,айко,honda,голос,современность,мимика,антропоморфность \n",
      "\n",
      "Tfidf filtered: робот,человек,человекоподобный робот,год,похожий человек,айко \n",
      "\n",
      "Эталон: робот,человекоподобный робот,антропоморфный робот,история,технология,внешность,функция\n"
     ]
    }
   ],
   "source": [
    "print('Rake filtered:', ','.join(rake_filtered[4]), '\\n')\n",
    "print('TextRank filtered:', ','.join(textrank_filtered[4]), '\\n')\n",
    "print('Tfidf filtered:', ','.join(tfidf_filtered[4]), '\\n')\n",
    "print('Эталон:', df['my_keywords'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f732af4",
   "metadata": {},
   "source": [
    "### Оценка без фильтров\n",
    "\n",
    "Посчитаем точность, полноту, F-меру для полученных тремя методами слов, к которым еще не был применен фильтр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c28d82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_set(selected, gold):\n",
    "    a = set(selected)\n",
    "    b = set(gold)\n",
    "    precision = len(a & b) / len(a)\n",
    "    recall = len(a & b) / len(b)\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7767da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(method_words, gold):\n",
    "    all_prec, all_rec, all_f1 = 0, 0, 0\n",
    "    for i, words in enumerate(method_words):\n",
    "        precision, recall, f1 = evaluate_one_set(words, gold[i])\n",
    "        all_prec += precision\n",
    "        all_rec += recall\n",
    "        all_f1 += f1\n",
    "    return all_prec/len(gold), all_rec/len(gold), all_f1/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a50b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = [words.split(',') for words in df['my_keywords'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4ea572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE: \t\t precision 0.11 \t recall 0.25 \t F1 0.15\n",
      "TextRank: \t precision 0.12 \t recall 0.35 \t F1 0.18\n",
      "Tf-Idf: \t precision 0.22 \t recall 0.42 \t F1 0.28\n"
     ]
    }
   ],
   "source": [
    "prec, rec, f1 = evaluate(rake_words, gold)\n",
    "print(f'RAKE: \\t\\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')\n",
    "\n",
    "prec, rec, f1 = evaluate(textrank_words, gold)\n",
    "print(f'TextRank: \\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')\n",
    "\n",
    "prec, rec, f1 = evaluate(tfidf_words, gold)\n",
    "print(f'Tf-Idf: \\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cf870",
   "metadata": {},
   "source": [
    "* Лучшие результаты по всем параметрам показал Tf-idf - он вычленял достаточно нужных слов и мало ненужных. Возможно, повлияло и то, что я сама установила сколько слов мне нужно - всего 10, потому что знаю, что эталонных слов больше не бывает. \n",
    "* Изначально TextRank показал худшие результаты, потому что предлагал избыточное количество слов. У него была очень высокая полнота, но очень низкая точность. Кроме того, TextRank, по-видимому, не выделяет биграммы, что уменьшает точность. Но после того, как я уменьшила параметр ratio, количество предлагаемых слов уменьшилось, следовательно, точность увеличилась.\n",
    "* В итоге худший результат оказался у Rake. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b1569",
   "metadata": {},
   "source": [
    "### Оценка с фильтром\n",
    "\n",
    "У всех методов качество улучшилось, что было ожидаемо. \n",
    "\n",
    "Теперь F1 у TextRank и Rake одинаковы, но TextRank достигает этой цифры за счет предложения большего количества слов, а Rake за счет точности, в чем ему помогает умение выделять биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "befa1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank with filter: \t precision 0.19 \t recall 0.35 \t F1 0.24\n",
      "RAKE with filter: \t precision 0.23 \t recall 0.25 \t F1 0.24\n",
      "Tf-Idf with filter: \t precision 0.32 \t recall 0.42 \t F1 0.35\n"
     ]
    }
   ],
   "source": [
    "prec, rec, f1 = evaluate(textrank_filtered, gold)\n",
    "print(f'TextRank with filter: \\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')\n",
    "\n",
    "prec, rec, f1 = evaluate(rake_filtered, gold)\n",
    "print(f'RAKE with filter: \\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')\n",
    "\n",
    "prec, rec, f1 = evaluate(tfidf_filtered, gold)\n",
    "print(f'Tf-Idf with filter: \\t precision {prec:0.2} \\t recall {rec:0.2} \\t F1 {f1:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece8059",
   "metadata": {},
   "source": [
    "### Анализ\n",
    "\n",
    "Оценка набора ключевых слов довольно субъективна. Уверена, что нашлись бы люди, которым больше бы понравился набор, предложенный тремя автоматическими способами, чем мной. Я тоже готова согласиться с некоторым их словами. \n",
    "\n",
    "Основная проблема данных трех методов в том, что они предлагают очень много слов. Все же не все слова, которые часто встречаются являются ключевыми. Иногда некоторые слова труднее заменить на синонимы, поэтому они так часто употребляются. Возможно, с этой проблемой помогло бы считать косинусную близость между выделенными ключевыми словами. Слова, которые совсем далеки от остальных, скорее всего попали в набор по ошибке.  \n",
    "Например, в примере ниже сочетание \"минное поле\" выделилось как ключевое слово. Оно часто встречалось, но лишь потому что его никак нельзя заменить в рассказе. А вот к понятию робот может относится много других понятий. \n",
    "\n",
    "\n",
    "Мне, как человеку, свойственно выделять ключевые слова не с помощью подсчета. Иногда эталонные ключевые слова очень редко встречались в тексте, а появлялись лишь потому что я сопосбна к обобщению. Эти же методы не могут выдать слово, которое в тексте не встречалось, потому что не могут обобщить несколько понятий под одним термином. С этим мог бы помочь WordNet, из которого можно извлекать гиперонимы для слов.  \n",
    "Например, если бы у нас был текст про породы собак и их названия часто там употреблялись, а само слово \"собака\" не встречалось бы, то с помощью гиперонима некоторая система все равно смогла бы предложить это слово в качестве ключевого.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c9f955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rake filtered: минный поле,youtube,человек,робот,животное,общество \n",
      "\n",
      "TextRank filtered: робот,человек,google,youtube,контент,поле,пол,животное,компания,год,цензура,система,дарлинг,исследование,фильтрация \n",
      "\n",
      "Tfidf filtered: робот,youtube,google,фильтрация,зрение,видеоролик,контент,человек \n",
      "\n",
      "Эталон: робот,битва,цензура,эмпатия,сочувствие,youtube\n"
     ]
    }
   ],
   "source": [
    "print('Rake filtered:', ','.join(rake_filtered[3]), '\\n')\n",
    "print('TextRank filtered:', ','.join(textrank_filtered[3]), '\\n')\n",
    "print('Tfidf filtered:', ','.join(tfidf_filtered[3]), '\\n')\n",
    "print('Эталон:', df['my_keywords'].iloc[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
