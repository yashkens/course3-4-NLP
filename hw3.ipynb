{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовительная часть для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'C:\\\\Users\\\\Yana\\\\Desktop\\\\school\\\\NLP\\\\mallet-2.0.8\\\\bin\\\\mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для выбора оптимального количества топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_num_topics(mallet_path, corpus, id2word, data_lemmatized):\n",
    "    best_score = 0\n",
    "    best_num = 0\n",
    "    for i in range(1, 31, 2):\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word, random_seed=100)\n",
    "        coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "        coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "        if coherence_ldamallet > best_score:\n",
    "            best_score = coherence_ldamallet\n",
    "            best_num = i\n",
    "    return best_num, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num, best_score = choose_num_topics(mallet_path, corpus, id2word, data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 0.542053875860002)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_num, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лучшее качество получилось при 27 топиках, так что запускаем с этим числом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=best_num, id2word=id2word, random_seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamallet.show_topics(formatted=False, num_topics=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сделаем словарь с топиками и их словами, чтобы потом найти главный топик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for topic in topics:\n",
    "    pairs = topic[1]\n",
    "    topic_words = {}\n",
    "    for pair in pairs:\n",
    "        topic_words[pair[0]] = pair[1]\n",
    "    topic_dict[topic[0]] = topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['time', 'day', 'back', 'hear', 'call', 'long', 'work', 'line', 'week', 'month'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dict[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция поиска главной темы\n",
    "Суммирует веса встречающихся слов из топика в тексте и считает главным тот топик, где сумма получилась больше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_topic(text, topic_dict):\n",
    "    scores = Counter()\n",
    "    for word in text:\n",
    "        for i in range(len(topic_dict)):\n",
    "            if word in topic_dict[i].keys():\n",
    "                scores[i] += topic_dict[i][word]\n",
    "    return scores.most_common(1), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mains = []\n",
    "for text in data_lemmatized:\n",
    "    main_topic, scores = find_main_topic(text, topic_dict)\n",
    "    if main_topic == []:\n",
    "        all_mains.append('None')\n",
    "    else:\n",
    "        all_mains.append(main_topic[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 13, 13, 13, 15, 15, 13, 13, 23, 13]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mains[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, большая часть текстов имеет главный топик 13, что подозрительно. Наверное, вычислять главный топик не так просто, как написано в описании домашки, и просто суммировать недостаточно. Поэтому я просто воспользуюсь встроенными возможностями маллета..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ldamallet[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mains_mallet = []\n",
    "for text in t:\n",
    "    main_topic = text[0][0]\n",
    "    all_mains_mallet.append(main_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 17, 4, 16, 15, 12, 3, 13, 25, 1]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mains_mallet[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так получше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сделаем датафрейм, чтобы разделить на группы по топикам и посчитаем тф-идф\n",
    "Сначала в датафрейме будут не нормальные тексты, а лемматизированные для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>main_topic_mallet</th>\n",
       "      <th>main_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[where, thing, car, nntp_poste, host, park, li...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[poll, final, call, summary, final, call, cloc...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[engineering, computer, network, distribution_...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[division, line, host, write, write, article, ...</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[question, distribution, article, write, clear...</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lemmatized_text  main_topic_mallet  \\\n",
       "0  [where, thing, car, nntp_poste, host, park, li...                  1   \n",
       "1  [poll, final, call, summary, final, call, cloc...                  6   \n",
       "2  [engineering, computer, network, distribution_...                  6   \n",
       "3  [division, line, host, write, write, article, ...                 23   \n",
       "4  [question, distribution, article, write, clear...                 19   \n",
       "\n",
       "  main_topic  \n",
       "0         13  \n",
       "1         13  \n",
       "2         13  \n",
       "3         13  \n",
       "4         13  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['lemmatized_text'] = data_lemmatized\n",
    "df['main_topic_mallet'] = all_mains_mallet\n",
    "df['main_topic'] = all_mains\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем IDF для каждого топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    idfDict = {}\n",
    "\n",
    "    all_count_dict = {}\n",
    "    for doc in documents:\n",
    "        for word in doc.keys():\n",
    "            if doc[word] != 0:\n",
    "                if word in all_count_dict:\n",
    "                    all_count_dict[word] += 1\n",
    "                else:\n",
    "                    all_count_dict[word] = 1\n",
    "    for word in all_count_dict.keys():\n",
    "        idfDict[word] = math.log(len(documents)/all_count_dict[word])\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counter(text):\n",
    "    text_counter = Counter()\n",
    "    for word in text:\n",
    "        text_counter[word] += 1\n",
    "    return text_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_idfs = {}\n",
    "for i in range(len(topic_dict)):\n",
    "    topic_texts = df.loc[df['main_topic_mallet'] == i, 'lemmatized_text']\n",
    "    counter_documents = []\n",
    "    for text in topic_texts:\n",
    "        text_counter = get_word_counter(text)\n",
    "        counter_documents.append(text_counter)\n",
    "    idf = computeIDF(counter_documents)\n",
    "    topic_idfs[i] = idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем tf-idf для каждого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(text_counter, text_list):\n",
    "    tfDict = {}\n",
    "    for word in text_counter.keys():\n",
    "        tfDict[word] = text_counter[word]/len(text_list)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_tfidfs = []\n",
    "for i in range(len(texts)):\n",
    "    text_counter = get_word_counter(texts[i])\n",
    "    tfDict = computeTF(text_counter, texts[i])\n",
    "    \n",
    "    topic_num = df.iloc[i]['main_topic_mallet']\n",
    "    idf = topic_idfs[topic_num]\n",
    "    tfidf = computeTFIDF(tfDict, idf)\n",
    "    \n",
    "    best = Counter(tfidf).most_common(5)\n",
    "    best_words_only = [word[0] for word in best]\n",
    "    all_best_tfidfs.append(best_words_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bricklin', 'tellme', 'lerxst', 'door', 'where'],\n",
       " ['poll', 'final', 'clock', 'day', 'acceleration'],\n",
       " ['bunch', 'display', 'powerbook', 'hear', 'appearence'],\n",
       " ['division', 'chip', 'quadrilateral', 'fill', 'weitek'],\n",
       " ['warn', 'error', 'unexpected', 'dumb', 'parity_error'],\n",
       " ['weapon', 'needless', 'individual', 'modern', 'term'],\n",
       " ['treatment', 'tumor', 'astrocytoma', 'accidentally', 'glad'],\n",
       " ['scsi', 'range', 'esdi', 'chip', 'indeed'],\n",
       " ['icon', 'win', 'appreciated', 'figure', 'wallpaper'],\n",
       " ['board', 'autodoubler', 'diskdoubler', 'licensing', 'icon']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_best_tfidfs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сделаем табличку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>main_topic_mallet</th>\n",
       "      <th>best TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bricklin, tellme, lerxst, door, where]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll - Final...</td>\n",
       "      <td>6</td>\n",
       "      <td>[poll, final, clock, day, acceleration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "      <td>6</td>\n",
       "      <td>[bunch, display, powerbook, hear, appearence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 ? ...</td>\n",
       "      <td>23</td>\n",
       "      <td>[division, chip, quadrilateral, fill, weitek]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "      <td>19</td>\n",
       "      <td>[warn, error, unexpected, dumb, parity_error]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: (Foxvog Douglas) Subject: Re: Rewording ...</td>\n",
       "      <td>26</td>\n",
       "      <td>[weapon, needless, individual, modern, term]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: (brian manning delaney) Subject: Brain T...</td>\n",
       "      <td>4</td>\n",
       "      <td>[treatment, tumor, astrocytoma, accidentally, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: (GRUBB) Subject: Re: IDE vs SCSI Organiz...</td>\n",
       "      <td>6</td>\n",
       "      <td>[scsi, range, esdi, chip, indeed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: Subject: WIn 3.0 ICON HELP PLEASE! Organ...</td>\n",
       "      <td>16</td>\n",
       "      <td>[icon, win, appreciated, figure, wallpaper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: (Stan Kerr) Subject: Re: Sigma Designs D...</td>\n",
       "      <td>6</td>\n",
       "      <td>[board, autodoubler, diskdoubler, licensing, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: (Irwin Arnstein) Subject: Re: Recommenda...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ducati, mate, computrac_inc, gts, bronze]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>From: (David Bold) Subject: Re: Question for t...</td>\n",
       "      <td>5</td>\n",
       "      <td>[parent, swear, child, multiple, code]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>From: (Rod Cerkoney) Subject: *$G4qxF,fekVH6 N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[fekvh, regard, rod, cerkoney, co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>From: (David B. Mckissock) Subject: Re: Space ...</td>\n",
       "      <td>22</td>\n",
       "      <td>[option, power, capability, module, acrv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>From: (Johnny L Lee) Subject: RE: == MOVING SA...</td>\n",
       "      <td>6</td>\n",
       "      <td>[purchase, move, portable, reasonable, decker]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lemmatized_text  main_topic_mallet  \\\n",
       "0   From: (wheres my thing) Subject: WHAT car is t...                  1   \n",
       "1   From: (Guy Kuo) Subject: SI Clock Poll - Final...                  6   \n",
       "2   From: (Thomas E Willis) Subject: PB questions....                  6   \n",
       "3   From: (Joe Green) Subject: Re: Weitek P9000 ? ...                 23   \n",
       "4   From: (Jonathan McDowell) Subject: Re: Shuttle...                 19   \n",
       "5   From: (Foxvog Douglas) Subject: Re: Rewording ...                 26   \n",
       "6   From: (brian manning delaney) Subject: Brain T...                  4   \n",
       "7   From: (GRUBB) Subject: Re: IDE vs SCSI Organiz...                  6   \n",
       "8   From: Subject: WIn 3.0 ICON HELP PLEASE! Organ...                 16   \n",
       "9   From: (Stan Kerr) Subject: Re: Sigma Designs D...                  6   \n",
       "10  From: (Irwin Arnstein) Subject: Re: Recommenda...                  1   \n",
       "11  From: (David Bold) Subject: Re: Question for t...                  5   \n",
       "12  From: (Rod Cerkoney) Subject: *$G4qxF,fekVH6 N...                  2   \n",
       "13  From: (David B. Mckissock) Subject: Re: Space ...                 22   \n",
       "14  From: (Johnny L Lee) Subject: RE: == MOVING SA...                  6   \n",
       "\n",
       "                                          best TF-IDF  \n",
       "0             [bricklin, tellme, lerxst, door, where]  \n",
       "1             [poll, final, clock, day, acceleration]  \n",
       "2       [bunch, display, powerbook, hear, appearence]  \n",
       "3       [division, chip, quadrilateral, fill, weitek]  \n",
       "4       [warn, error, unexpected, dumb, parity_error]  \n",
       "5        [weapon, needless, individual, modern, term]  \n",
       "6   [treatment, tumor, astrocytoma, accidentally, ...  \n",
       "7                   [scsi, range, esdi, chip, indeed]  \n",
       "8         [icon, win, appreciated, figure, wallpaper]  \n",
       "9   [board, autodoubler, diskdoubler, licensing, i...  \n",
       "10         [ducati, mate, computrac_inc, gts, bronze]  \n",
       "11             [parent, swear, child, multiple, code]  \n",
       "12                 [fekvh, regard, rod, cerkoney, co]  \n",
       "13          [option, power, capability, module, acrv]  \n",
       "14     [purchase, move, portable, reasonable, decker]  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['lemmatized_text'] = data\n",
    "df_result['main_topic_mallet'] = all_mains_mallet\n",
    "df_result['best TF-IDF'] = all_best_tfidfs\n",
    "df_result.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
