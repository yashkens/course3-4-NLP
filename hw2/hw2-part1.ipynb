{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "import natasha\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "import nltk\n",
    "import spacy\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Русский\n",
    "Текст для проверки качества состоит из предложений, содержащий следующие проблемные места для разметки:\n",
    "* Различные омонимы: шум стрих, почитать стих\n",
    "* Аббревиатуры, сокращения: Росагроэкспорт, МГУ, т.е.\n",
    "* Имена собственные: необычные имена и названия\n",
    "* Сложные слова: мегапроизводительный, двадцатичетырехчасовой\n",
    "* Редкие формы слов: странные императивы и деепричастия\n",
    "* Редкие слова: заимстовования, сленг, диалектные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем ru_text - текст без моей разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('russian-test.txt', 'r', encoding='utf-8') as f:\n",
    "    ru_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для извлечения правильных ответов разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mark_answers(filename):\n",
    "    answer = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        marked = f.read()\n",
    "    lines = marked.split('\\n')\n",
    "    for line in lines:\n",
    "        answer.append(line.split('\\t')[1])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_answer = get_mark_answers('ru_marked.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция создает словарь соотвествий тегов, чтобы потом все унифицировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pos_names(filename):\n",
    "    pos_names_dict = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        pos_names_file = f.read()\n",
    "    pos_names = pos_names_file.split('\\n')\n",
    "    for name in pos_names:\n",
    "        parts = name.split('\\t')\n",
    "        for pos in parts[1].split(','):\n",
    "            pos_names_dict[pos] = parts[0]\n",
    "    return pos_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_names_ru = get_all_pos_names('pos_names.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции ниже унифицируют теги. Две из них для определенных систем разметки, потому что там есть неоднозначности, а последняя функция пригодится больше одного раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_pos_mystem(predict_pos, pos_names):\n",
    "    for i in range(len(predict_pos)):\n",
    "        pos = predict_pos[i].split(',')[0].split('=')[0]\n",
    "        \n",
    "        if 'cравн' in predict_pos[i]:\n",
    "            predict_pos[i] = 'COMP'\n",
    "        elif 'прич' in predict_pos[i]:\n",
    "            predict_pos[i] = 'ADJ'\n",
    "        elif pos in pos_names.keys():\n",
    "            predict_pos[i] =  pos_names[pos]\n",
    "        else:\n",
    "            predict_pos[i] = pos\n",
    "    return predict_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_pos_natasha(predict_pos, pos_names):\n",
    "    for i in range(len(predict_pos)):\n",
    "        pos = predict_pos[i].pos\n",
    "        \n",
    "        if 'Cmp' in predict_pos[i].feats.values():\n",
    "            predict_pos[i] = 'COMP'\n",
    "        elif predict_pos[i].pos == 'PRON' and 'Gender' in predict_pos[i].feats and 'Person' not in predict_pos[i].feats and 'Animacy' not in predict_pos[i].feats:\n",
    "            predict_pos[i] = 'ADJ'\n",
    "        elif pos in pos_names.keys():\n",
    "            predict_pos[i] =  pos_names[pos]\n",
    "        else:\n",
    "            predict_pos[i] = pos\n",
    "    return predict_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_unify_pos(predict_pos, pos_names):\n",
    "    for i in range(len(predict_pos)):\n",
    "        if predict_pos[i] in pos_names.keys():\n",
    "            predict_pos[i] =  pos_names[predict_pos[i]]\n",
    "    return predict_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = simple_word_tokenize(ru_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_predict = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word[0].isalpha():\n",
    "        pymorphy_predict.append(str(morph.parse(word)[0].tag.POS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_pymorphy_predict = simple_unify_pos(pymorphy_predict, pos_names_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "print(\"Pymorphy accuracy: %.4f\" % accuracy_score(unified_pymorphy_predict, ru_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Майстемовский токенизатор плохо обращается с дефисами, поэтому пришлось как-то регулировать количество ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftover = 'то'\n",
    "mystem_predict = []\n",
    "ana = m.analyze(ru_text)\n",
    "for word in ana:\n",
    "    if word != {'text': ' '} and word['text'][0].isalpha() and word['text'] != leftover:\n",
    "        if len(word['analysis']) == 0:\n",
    "            mystem_predict.append('None')\n",
    "        else:\n",
    "            gram = word['analysis'][0]['gr']\n",
    "            mystem_predict.append(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_mystem_predict = unify_pos_mystem(mystem_predict, pos_names_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mystem accuracy: 0.8613\n"
     ]
    }
   ],
   "source": [
    "print(\"Mystem accuracy: %.4f\" % accuracy_score(unified_mystem_predict, ru_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_natasha_predict = unify_pos_natasha(doc.tokens, pos_names_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7810\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(unified_natasha_predict, ru_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Английский\n",
    "Текст для проверки разметки английского в основном состоит из большого количества разных омонимов (пары прил.-сущ., гл.-сущ, ing формы и другое). \n",
    "Еще я добавила в текст аббревиатуры, заимствованные слова, сленг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english-test.txt', 'r', encoding='utf-8') as f:\n",
    "    en_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_answer.txt', 'r', encoding='utf-8') as f:\n",
    "    en_marked = f.read()\n",
    "    \n",
    "en_answer = []\n",
    "\n",
    "lines = en_marked.split('\\n')\n",
    "for line in lines:\n",
    "    en_answer.append(line.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_pos_names.txt', 'r', encoding='utf-8') as f:\n",
    "    en_pos_names = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pos_names = get_all_pos_names('en_pos_names.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(en_text)\n",
    "nltk_pos = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_predict = []\n",
    "for pos in nltk_pos:\n",
    "    if pos[0][0].isalpha():\n",
    "        nltk_predict.append(pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_nltk_predict = simple_unify_pos(nltk_predict, en_pos_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK accuracy: 0.8324\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK accuracy: %.4f\" % accuracy_score(unified_nltk_predict, en_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_predict = []\n",
    "doc = nlp(en_text)\n",
    "for s in doc.sents:\n",
    "    for t in s:\n",
    "        if t.text[0].isalpha():\n",
    "            spacy_predict.append(t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_spacy_predict = simple_unify_pos(spacy_predict, en_pos_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy accuracy: 0.9017\n"
     ]
    }
   ],
   "source": [
    "print(\"Spacy accuracy: %.4f\" % accuracy_score(unified_spacy_predict, en_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-11 19:56:06,678 loading file C:\\Users\\Yana\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Sentence(en_text)\n",
    "tagger.predict(text)\n",
    "tagged_text = text.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_predict = []\n",
    "pairs = tagged_text.split('> ')\n",
    "for pair in pairs:\n",
    "    part = pair.split()\n",
    "    if part[0][0].isalpha():\n",
    "          flair_predict.append(part[1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_flair_predict = simple_unify_pos(flair_predict, en_pos_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8960\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(unified_flair_predict, en_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
