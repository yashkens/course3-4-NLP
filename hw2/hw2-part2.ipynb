{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "from sklearn.metrics import accuracy_score\n",
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтобы найти новую часть, пролистайте ниже. Там большими буквами будет написано, не пропустите!\n",
    "p.s. часть из дз1 немного отличается по двум причинам: сохранять только словарь частотных слов теперь недостаточно, поэтому еще сохранялись тесты целиком, а также возникли проблемы с блокировкой меня сайтом, но это уже не так важно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция ниже получает код страницы для поиска тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(link_part):\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    link = 'https://irecommend.ru' + link_part\n",
    "    response = session.get(link, headers=headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для выбора слов и добавления их в словарь\n",
    "Берутся тексты длиной более 200 слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала я составляла обычные каунтеры, но тогда результат получался плохой - получалось очень много названий конкретных фильмов и имен. Я связываю это с тем, что некоторые отзывы были значительно больше остальных, поэтому слова из этих отзывов значительно перевешивали. Чтобы попробовать от этого избавиться, я считаю частотность по тексту (кол-во этот слова/кол-во всех слов в тексте) и ее прибавляю к текущему значению в словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_lemmas(text, freq, count):\n",
    "    counter = Counter()\n",
    "    text = text.text\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
    "    if len(lemmas) > 200:\n",
    "        count += 1\n",
    "        for lemma in lemmas:\n",
    "            if lemma.isalpha():\n",
    "                counter[lemma] += 1\n",
    "    for word in dict(counter).keys():\n",
    "        freq[word] += counter[word]/len(lemmas)\n",
    "    return freq, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция собирает не менее 30 отзывов и собирает словарь\n",
    "По каждому фильму не больше 2 плохих и 2 хороших отзывов, чтобы не было перевеса слов, относящихся к определенному жанру или фильму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = []\n",
    "g_list = []\n",
    "b_freq = Counter()\n",
    "g_freq = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(link):\n",
    "    b_count_all = 0\n",
    "    g_count_all = 0\n",
    "    global b_list\n",
    "    global g_list\n",
    "    global b_freq\n",
    "    global g_freq\n",
    "\n",
    "    \n",
    "    soup = get_soup(link)\n",
    "    all_films = soup.find_all('a', {'class': 'read-all-reviews-link-bottom read-all-reviews-link'})\n",
    "    \n",
    "    for film in all_films:\n",
    "        b_count = 0\n",
    "        g_count = 0\n",
    "        if b_count_all > 30 and g_count_all > 30:\n",
    "            break\n",
    "        sleep(10)\n",
    "        print(film.get('href'))\n",
    "        soup = get_soup(film.get('href'))\n",
    "        all_reviews = soup.find_all('a', {'class': 'more'})\n",
    "        \n",
    "        for review in all_reviews:\n",
    "            sleep(10)\n",
    "            soup = get_soup(review.get('href'))\n",
    "            recommend = soup.find('span', {'class': 'verdict'})\n",
    "            \n",
    "            if recommend.text == 'не рекомендует' and b_count <= 2 and b_count_all <= 30:\n",
    "                text = soup.find('div', {'class': 'description hasinlineimage'})\n",
    "                b_list.append(text.text)\n",
    "                b_freq, b_count = get_text_lemmas(text, b_freq, b_count)\n",
    "                b_count_all += b_count\n",
    "                \n",
    "            elif g_count <= 2 and g_count_all <= 30:\n",
    "                text = soup.find('div', {'class': 'description hasinlineimage'})\n",
    "                g_list.append(text.text)\n",
    "                g_freq, g_count = get_text_lemmas(text, g_freq, g_count)\n",
    "                g_count_all += g_count\n",
    "                \n",
    "            elif g_count > 2 and b_count > 2:\n",
    "                break\n",
    "                \n",
    "    return b_freq, g_freq, b_count_all, g_count_all, g_list, b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Собираем словари и смотрим содержимое\n",
    "Было собрано по 33 отзыва каждой категории. Очень много частотных слов, списки ни о чем не говорят."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/unesennye-vetrom-gone-wind\n",
      "/content/belosnezhka-skazka-dlya-vzroslykh-blanche-comme-neige\n",
      "/content/gattaka\n",
      "/content/neuyazvimyi-unbreakable\n",
      "/content/strakh-glubiny-deep\n"
     ]
    }
   ],
   "source": [
    "link = '/category/filmy?page=1'\n",
    "bad_freq, good_freq, b_count_all, g_count_all, g_list, b_list = find_words(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_freq1 = good_freq\n",
    "b_freq1 = bad_freq\n",
    "g_list1 = g_list\n",
    "b_list1 = b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list.extend(g_list1)\n",
    "b_list.extend(b_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_freq(freq1, freq2):\n",
    "    for f in freq1.keys():\n",
    "        if f in freq2.keys():\n",
    "            freq2[f] += freq1[f]\n",
    "        else:\n",
    "            freq2[f] = freq1[f]\n",
    "    return freq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_g_freq = join_freq(good_freq, g_freq1)\n",
    "full_b_freq = join_freq(bad_freq, b_freq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_freq = select_words(full_b_freq)\n",
    "new_good_freq = select_words(full_g_freq)\n",
    "pos_freq, neg_freq = delete_common_words(new_good_freq, new_bad_freq)\n",
    "adj_neg = get_adj(neg_freq)\n",
    "adj_pos = get_adj(pos_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('непонятный', 0.012634654589946833),\n",
       " ('неприятный', 0.012463869882587325),\n",
       " ('ходячий', 0.009478672985781991),\n",
       " ('смешной', 0.008785445722234839),\n",
       " ('банальный', 0.007743268876872116),\n",
       " ('гадкий', 0.007679180887372013),\n",
       " ('смазливый', 0.007380073800738007),\n",
       " ('средний', 0.007344415957672649),\n",
       " ('никакой', 0.007079809476715317),\n",
       " ('животное', 0.006947607489814127),\n",
       " ('конкретный', 0.006674731748369451),\n",
       " ('голый', 0.006560572947127568),\n",
       " ('следующий', 0.005972816336137845),\n",
       " ('китайский', 0.005641041486425246),\n",
       " ('противный', 0.005119453924914676)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_neg.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('неуязвимый', 0.12949969358042082),\n",
       " ('идеальный', 0.043395355387284916),\n",
       " ('приятный', 0.030959912258421974),\n",
       " ('нормальный', 0.03042915279319275),\n",
       " ('страшный', 0.029386409334314995),\n",
       " ('научный', 0.029156259521878135),\n",
       " ('золотой', 0.028009698783410704),\n",
       " ('разумный', 0.022236655462407832),\n",
       " ('тёмный', 0.02222037168451984),\n",
       " ('местный', 0.021839620781679177),\n",
       " ('естественный', 0.021418031260297128),\n",
       " ('невероятный', 0.02080485896478325),\n",
       " ('книжный', 0.020753336014244415),\n",
       " ('опасный', 0.0191455839241667),\n",
       " ('генный', 0.01892147587511826)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_pos.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция удаляет стоп-слова\n",
    "Я не стала отбирать слова по частотности, потому что не ясно, где устанавливать нижнюю границу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_words(freq):\n",
    "    stopWords = set(stopwords.words('russian'))\n",
    "    selected_freq = Counter()\n",
    "    for word in dict(freq).keys():\n",
    "        if word not in stopWords:\n",
    "            selected_freq[word] = freq[word]\n",
    "    return selected_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция удаляет общие для двух словарей слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_common_words(good_freq, bad_freq):\n",
    "    good = dict(good_freq)\n",
    "    bad = dict(bad_freq)\n",
    "    keys = list(bad.keys())\n",
    "    for b_key in keys:\n",
    "        if b_key in good.keys():\n",
    "            del bad[b_key]\n",
    "            del good[b_key]\n",
    "    return Counter(good), Counter(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это мы уже сделали \n",
    "\n",
    "# new_bad_freq = select_words(bad_freq)\n",
    "# new_good_freq = select_words(good_freq)\n",
    "# pos_freq, neg_freq = delete_common_words(new_good_freq, new_bad_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в списках очень много каких-то случайных существительных. Я решила, что будет намного показательнее, если в списках останутся только прилагательные. Это должно увеличить точность оценки, даже несмотря на то, что некоторые \"полезные\" существительные и глаголы уберутся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для отбора прилагательных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(freq):\n",
    "    result = Counter()\n",
    "    for word in dict(freq).keys():\n",
    "        if morph.parse(word)[0].tag.POS == 'ADJF':\n",
    "            result[word] = freq[word]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это тоже сделали \n",
    "\n",
    "# adj_neg = get_adj(neg_freq)\n",
    "# adj_pos = get_adj(pos_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_neg.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_pos.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились относительно нормальные списки с наличием подходящих оценочных слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция собирает тестовую выборку\n",
    "Скачивается не менее 10 отзывов с их оценкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_link = 'https://irecommend.ru/category/filmy?page=499'\n",
    "\n",
    "def get_test_texts(n):\n",
    "    text_dict = {}\n",
    "    b_count_all = 0\n",
    "    g_count_all = 0\n",
    "\n",
    "    soup = get_soup('/category/filmy?page=499')\n",
    "    all_books = soup.find_all('a', {'class': 'read-all-reviews-link-bottom read-all-reviews-link'})\n",
    "\n",
    "    for book in all_books:\n",
    "        b_count = 0\n",
    "        g_count = 0\n",
    "        if b_count_all > n and g_count_all > n:\n",
    "            break\n",
    "        sleep(10)\n",
    "        soup = get_soup(book.get('href'))\n",
    "        all_reviews = soup.find_all('a', {'class': 'more'})\n",
    "        \n",
    "        for review in all_reviews:\n",
    "            sleep(10)\n",
    "            soup = get_soup(review.get('href'))\n",
    "            recommend = soup.find('span', {'class': 'verdict'})\n",
    "            \n",
    "            if recommend.text == 'не рекомендует' and b_count <= 2 and b_count_all <= n:\n",
    "                text = soup.find('div', {'class': 'description hasinlineimage'})\n",
    "                text_dict[text.text] = recommend.text\n",
    "                b_count += 1\n",
    "                b_count_all += b_count\n",
    "                \n",
    "            elif g_count <= 2 and g_count_all <= n:\n",
    "                text = soup.find('div', {'class': 'description hasinlineimage'})\n",
    "                text_dict[text.text] = recommend.text\n",
    "                g_count += 1\n",
    "                g_count_all += g_count\n",
    "                \n",
    "            elif g_count > 2 and b_count > 2:\n",
    "                break\n",
    "                \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = get_test_texts(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Новая часть! ДЗ2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первой части дз2 Рymorphy покзал лучшие результаты, поэтому беру его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция составляет словарь с биграммами вида \"не\" + любое слово\n",
    "Частица \"не\" имеет особенно важное значение для определения негативных отзывов, поэтому я взяла такой биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neg_pair(pos_tagged, neg_pairs):\n",
    "    counter = Counter()\n",
    "    for i in range(len(pos_tagged)-1):\n",
    "        if list(pos_tagged[i].keys())[0].lower() == 'не':\n",
    "            pair = 'не ' + list(pos_tagged[i+1].keys())[0].lower()\n",
    "            counter[pair] += 1\n",
    "    \n",
    "    for word in dict(counter).keys():\n",
    "        neg_pairs[word] += counter[word]/len(pos_tagged)\n",
    "    return neg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция составляет словарь с биграммами из двух заданных частей речи\n",
    "Я выбрала следующие биграммы:\n",
    "* Прилагательное + существительное - прилагательные часто используются именно для оценки предметов и понятий, поэтому такой биграмм может быть показательным\n",
    "* Глагол + существительное - это должна быть модель глагола и его объекта, которая покажет пары типа \"понравилась идея\", \"оценила сюжет\" и т.п.\n",
    "* Существительное + глагол - эта модель должна включать оценки того, как фильм повлиял на эмоции человека: \"фильм впечатлил\", \"сюжет разочаровал\" и т.п.\n",
    "\n",
    "Эксперимернтальным методом выяснилось, что последние два типа биграммов не улучшают качество, а наоборот ухудшают, поэтому они не будут добавлены..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pos_pair(pos_tagged, pos1, pos2, pairs):\n",
    "    counter = Counter()\n",
    "    for i in range(len(pos_tagged)-1):\n",
    "        if list(pos_tagged[i].values())[0] == pos1 and list(pos_tagged[i+1].values())[0] == pos2:\n",
    "            pair = list(pos_tagged[i].keys())[0].lower() + ' ' + list(pos_tagged[i+1].keys())[0].lower()\n",
    "            counter[pair] += 1\n",
    "            \n",
    "    for word in dict(counter).keys():\n",
    "        pairs[word] += counter[word]/len(pos_tagged)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция находит все нужные мне биграммы в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(text_list):\n",
    "    neg_pairs = Counter()\n",
    "    obj = Counter()\n",
    "    subj = Counter()\n",
    "    adj_noun = Counter()\n",
    "    for text in text_list:\n",
    "        pos_tagged = []\n",
    "        tokens = simple_word_tokenize(text.lower())\n",
    "        lemmas = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "        for word in lemmas:\n",
    "            if word[0].isalpha():\n",
    "                pos_tagged.append({word:str(morph.parse(word)[0].tag.POS)})\n",
    "        \n",
    "        neg_pairs = find_neg_pair(pos_tagged, neg_pairs)\n",
    "        obj = find_pos_pair(pos_tagged, 'INFN', 'NOUN', obj)\n",
    "        subj = find_pos_pair(pos_tagged, 'NOUN', 'INFN', subj)\n",
    "        adj_noun = find_pos_pair(pos_tagged, 'ADJF', 'NOUN', adj_noun)\n",
    "        \n",
    "    return neg_pairs, obj, subj, adj_noun\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_neg, b_obj, b_subj, b_adj_noun = find_ngrams(b_list)\n",
    "g_neg, g_obj, g_subj, g_adj_noun = find_ngrams(g_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pairs = b_neg\n",
    "# bad_pairs.update(b_obj)\n",
    "# bad_pairs.update(b_subj)\n",
    "bad_pairs.update(b_adj_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pairs = g_neg\n",
    "# good_pairs.update(g_obj)\n",
    "# good_pairs.update(g_subj)\n",
    "good_pairs.update(g_adj_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_pairs = select_pairs(bad_pairs)\n",
    "new_good_pairs = select_pairs(good_pairs)\n",
    "pos_pairs, neg_pairs = delete_common_words(new_good_pairs, new_bad_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('звёздный война', 0.0323936690811007),\n",
       " ('не пожалеть', 0.024574369690326118),\n",
       " ('весь часть', 0.02247191011235955),\n",
       " ('тёмный сторона', 0.02222321855571497),\n",
       " ('приятный просмотр', 0.018682506204139752),\n",
       " ('эротический мелодрама', 0.01680672268907563),\n",
       " ('семейный пара', 0.015411319166841995),\n",
       " ('замечательный фильм', 0.01242785016940981),\n",
       " ('хороший часть', 0.011806551889048302),\n",
       " ('реальный событие', 0.011366557905013785),\n",
       " ('не решаться', 0.011235955056179775),\n",
       " ('не откладывать', 0.011235955056179775),\n",
       " ('прекрасный момент', 0.011235955056179775),\n",
       " ('интересный герой', 0.011235955056179775),\n",
       " ('весь сага', 0.010741047358254263)]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pairs.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не рекомендовать', 0.026928894626102626),\n",
       " ('положительный отзыв', 0.014616429457272782),\n",
       " ('не вдохновить', 0.012195121951219513),\n",
       " ('не глупый', 0.012195121951219513),\n",
       " ('хвалебный речь', 0.012195121951219513),\n",
       " ('внутренний дрожь', 0.012195121951219513),\n",
       " ('глупый кокетка', 0.012195121951219513),\n",
       " ('сильный человек', 0.012195121951219513),\n",
       " ('сильный книга', 0.012195121951219513),\n",
       " ('не понравиться', 0.011920233243866598),\n",
       " ('не притягивать', 0.008403361344537815),\n",
       " ('несгибаемый характер', 0.008403361344537815),\n",
       " ('безответственный эгоистка', 0.008403361344537815),\n",
       " ('каждый минута', 0.008403361344537815),\n",
       " ('мировой шедевр', 0.008403361344537815)]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_pairs.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция отбирает нужные биграммы\n",
    "Убираются биграммы, где одно слово является стопсловом (не считаем \"не\"), чтобы убрать бесполезные биграммы вроде \"не так\". Еще устанавливается нижняя граница частотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pairs(freq):\n",
    "    stopWords = set(stopwords.words('russian'))\n",
    "    stopWords.remove('не')\n",
    "    selected_freq = Counter()\n",
    "    for word in dict(freq).keys():\n",
    "        if word.split()[0] not in stopWords and word.split()[1] not in stopWords and freq[word] > 0.001:\n",
    "            selected_freq[word] = freq[word]\n",
    "    return selected_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тут еще кусочек старой домашки - оценка результатов\n",
    "Но это нужно посмотреть, чтобы сравнить результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, предсказывающая оценку\n",
    "Итоговые положительные и отрицательные очки делятся на суммы всех значений в соответствующих словарях, чтобы получить процентные соотношения. Нельзя сравнивать очки, потому что суммы значений в разных оценках могут отличаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_verdict(text, adj_pos, adj_neg):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
    "    for lemma in lemmas:\n",
    "        if lemma in adj_pos.keys():\n",
    "            pos_score += adj_pos[lemma]\n",
    "        elif lemma in adj_neg.keys():\n",
    "            neg_score += adj_neg[lemma]\n",
    "    n_score = neg_score/sum(adj_neg.values())\n",
    "    p_score = pos_score/sum(adj_pos.values())\n",
    "    return p_score, n_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на совпадения предсказаний и правильных ответов. Так получилось, что в 3 текстах вообще не встретились слова из собранных словарей, поэтому предсказать оценку невозможно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in test_dict.keys():\n",
    "#     p_score, n_score = predict_verdict(text, adj_pos, adj_neg)\n",
    "#     if p_score > n_score:\n",
    "#         print('predict: positive', p_score, n_score)\n",
    "#     elif p_score == n_score:\n",
    "#         print('predict: can not predict')\n",
    "#     else:\n",
    "#         print('predict: negative', p_score, n_score)\n",
    "#     print('answer:', test_dict[text], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем accuracy. Получилось 77%. В прошлый раз получилось 75%, но сейчас набор текстов новый, поэтому есть небольшое отличие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7692\n"
     ]
    }
   ],
   "source": [
    "# 1 - pos, 0 - neg\n",
    "predict = []\n",
    "gold = []\n",
    "for text in test_dict.keys():\n",
    "    p_score, n_score = predict_verdict(text, adj_pos, adj_neg)\n",
    "    if p_score > n_score:\n",
    "        predict.append(1)\n",
    "    else:\n",
    "        predict.append(0)\n",
    "    if test_dict[text] == 'рекомендует':\n",
    "        gold.append(1)\n",
    "    else:\n",
    "        gold.append(0)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(predict, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция предсказания оценки, принимающая во внимание собранные биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_predict_verdict(text, adj_pos, adj_neg, pos_pairs, neg_pairs):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    pos_score_p = 0\n",
    "    neg_score_p = 0\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
    "    for lemma in lemmas:\n",
    "        if lemma in adj_pos.keys():\n",
    "            pos_score += adj_pos[lemma]\n",
    "        elif lemma in adj_neg.keys():\n",
    "            neg_score += adj_neg[lemma]\n",
    "    for i in range(len(lemmas)-1):\n",
    "        if lemmas[i] + ' ' + lemmas[i+1] in pos_pairs.keys():\n",
    "            pos_score_p += pos_pairs[lemmas[i] + ' ' + lemmas[i+1]]\n",
    "        elif lemmas[i] + ' ' + lemmas[i+1] in neg_pairs.keys():\n",
    "            neg_score_p += neg_pairs[lemmas[i] + ' ' + lemmas[i+1]]\n",
    "    n_score = neg_score/sum(adj_neg.values()) + neg_score_p/sum(neg_pairs.values())\n",
    "    p_score = pos_score/sum(adj_pos.values()) + pos_score_p/sum(pos_pairs.values())\n",
    "    return p_score, n_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "predict = []\n",
    "gold = []\n",
    "for text in test_dict.keys():\n",
    "    p_score, n_score = upd_predict_verdict(text, adj_pos, adj_neg, pos_pairs, neg_pairs)\n",
    "    if p_score > n_score:\n",
    "        predict.append(1)\n",
    "    else:\n",
    "        predict.append(0)\n",
    "    if test_dict[text] == 'рекомендует':\n",
    "        gold.append(1)\n",
    "    else:\n",
    "        gold.append(0)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(predict, gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось 92%. Качество улучшилось!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
